diff --git a/Dockerfile b/Dockerfile
index 733c9b8..4edf54e 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -104,7 +104,6 @@ RUN ls
 COPY requirements.txt requirements.txt
 RUN pip install -r requirements.txt
 
-# RUN conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
-
+#RUN conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia
 
 RUN ls
diff --git a/Dockerfile2 b/Dockerfile2
new file mode 100644
index 0000000..5c86989
--- /dev/null
+++ b/Dockerfile2
@@ -0,0 +1,111 @@
+# Base container that includes all dependencies but not the actual repo
+
+ARG UBUNTU_VERSION=20.04
+ARG ARCH=
+ARG CUDA=11.4.0
+
+# RUN echo nvidia/cudagl${ARCH:+-$ARCH}:${CUDA}-base-ubuntu${UBUNTU_VERSION}
+# FROM nvidia/cudagl${ARCH:+-$ARCH}:${CUDA}-base-ubuntu${UBUNTU_VERSION} as base
+#FROM nvidia/cudagl:11.4.2-base-ubuntu20.04 as base
+FROM liqingya/mujoco:dmc-atari-py36-torch1.7-cu110
+# ARCH and CUDA are specified again because the FROM directive resets ARGs
+# (but their default value is retained if set previously)
+
+ARG UBUNTU_VERSION
+ARG ARCH
+ARG CUDA
+ARG CUDNN=7.6.5.32-1
+
+SHELL ["/bin/bash", "-c"]
+
+ENV DEBIAN_FRONTEND="noninteractive"
+# See http://bugs.python.org/issue19846
+ENV LANG=C.UTF-8 LC_ALL=C.UTF-8
+ENV PATH /opt/conda/bin:$PATH
+
+# install anaconda
+RUN apt-get update --fix-missing && apt-get install -y wget bzip2 ca-certificates \
+    libglib2.0-0 libxext6 libsm6 libxrender1 \
+    git mercurial subversion
+    
+# NOTE: we don't use TF so might not need some of these
+# ========== Tensorflow dependencies ==========
+RUN apt-get update \
+    && apt-get install -y --no-install-recommends \
+        build-essential \
+        libfreetype6-dev \
+        libhdf5-serial-dev \
+        libzmq3-dev \
+        pkg-config \
+        software-properties-common \
+        zip \
+        unzip \
+    && apt-get clean \
+    && rm -rf /var/lib/apt/lists/*
+
+SHELL ["/bin/bash", "-c"]
+
+RUN apt-get update -y
+# RUN apt-get install -y python3-dev python3-pip
+RUN apt-get update --fix-missing
+RUN apt-get install -y wget bzip2 ca-certificates git vim
+RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
+        build-essential \
+        premake4 \
+        git \
+        curl \
+        vim \
+        ffmpeg \
+	    libgl1-mesa-dev \
+	    libgl1-mesa-glx \
+	    libglew-dev \
+	    libosmesa6-dev \
+	    libxrender-dev \
+	    libsm6 libxext6 \
+        unzip \
+        patchelf \
+        ffmpeg \
+        libxrandr2 \
+        libxinerama1 \
+        libxcursor1 \
+        python3-dev python3-pip graphviz \
+        freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev libglew1.6-dev mesa-utils
+        
+# Not sure why this is needed
+ENV LANG C.UTF-8
+
+# Not sure what this is fixing
+# COPY ./files/Xdummy /usr/local/bin/Xdummy
+# RUN chmod +x /usr/local/bin/Xdummy
+        
+ENV PATH /opt/conda/bin:$PATH
+RUN wget --quiet https://repo.anaconda.com/archive/Anaconda2-2019.10-Linux-x86_64.sh -O /tmp/miniconda.sh && \
+    /bin/bash /tmp/miniconda.sh -b -p /opt/conda && \
+    rm /tmp/miniconda.sh && \
+    ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh && \
+    echo ". /opt/conda/etc/profile.d/conda.sh" >> /etc/bash.bashrc
+
+RUN conda update -y --name base conda && conda clean --all -y
+
+RUN conda create --name roble python=3.7 pip
+RUN echo "source activate roble" >> ~/.bashrc
+## Make it so you can install things to the correct version of pip
+ENV PATH /opt/conda/envs/roble/bin:$PATH
+RUN source activate roble
+
+RUN mkdir /root/playground
+
+# make sure your domain is accepted
+# RUN touch /root/.ssh/known_hosts
+RUN mkdir /root/.ssh
+RUN ssh-keyscan github.com >> /root/.ssh/known_hosts
+
+RUN ls
+COPY requirements.txt requirements.txt
+RUN pip install -r requirements.txt
+
+RUN conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia
+
+RUN pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
+
+RUN ls
\ No newline at end of file
diff --git a/conf/config_hw1.yaml b/conf/config_hw1.yaml
index 7ba75d1..15075a4 100644
--- a/conf/config_hw1.yaml
+++ b/conf/config_hw1.yaml
@@ -1,34 +1,30 @@
-env: 
-  expert_policy_file: ../../../hw1/roble/policies/experts/Ant.pkl # Relative to where you're running this script from 
-  expert_data: ../../../hw1/roble/expert_data/expert_data_Ant-v2.pkl  # Relative to where you're running this script from
-  exp_name: "bob"
-  env_name: Ant-v2 # choices are [Ant-v2, Humanoid-v2, Walker2d-v2, HalfCheetah-v2, Hopper-v2]
-  max_episode_length: 100 
-  render: false
-  
+env:
+  expert_policy_file: ../../../hw1/roble/policies/experts/Walker2d.pkl
+  expert_data: ../../../hw1/roble/expert_data/expert_data_Walker2d-v2.pkl
+  exp_name: bob
+  env_name: Walker2d-v2
+  max_episode_length: 1000
+  render: true
 alg:
   num_rollouts: 5
   do_dagger: true
-  num_agent_train_steps_per_iter: 1000 # number of gradient steps for training policy (per iter in n_iter)
-  n_iter: 10
-  batch_size: 1000 # training data collected (in the env) during each iteration
-  eval_batch_size: 1000 # eval data collected (in the env) for logging metrics
-  train_batch_size: 100 # number of sampled data points to be used per gradient/train step
-  n_layers: 2 # Network depths
-  network_width: 64 # The width of the network layers
-  learning_rate: 5e-3 # THe learning rate for BC
-  max_replay_buffer_size: 1000000 ## Size of the replay buffer
-  use_gpu: False
-  which_gpu: 0 # The index for the GPU (the computer you use may have more than one)
-  discrete: False
-  ac_dim: 0 ## This will be overridden in the code
-  ob_dim: 0 ## This will be overridden in the code
+  num_agent_train_steps_per_iter: 1000
+  n_iter: 21
+  batch_size: 1000
+  eval_batch_size: 10000
+  train_batch_size: 1024
+  n_layers: 2
+  network_width: 64
+  learning_rate: 0.005
+  max_replay_buffer_size: 1000000
+  use_gpu: false
+  which_gpu: 0
+  discrete: false
+  ac_dim: 0
+  ob_dim: 0
 
 logging:
-  video_log_freq: 5 # How often to generate a video to log/
-  scalar_log_freq: 1 # How often to log training information and run evaluation during training.
-  save_params: true # Should the parameters given to the script be saved? (Always...)
+  video_log_freq: 5
+  scalar_log_freq: 1
+  save_params: true
   random_seed: 1234
-  
-  
-
diff --git a/hw1/conf/config.yaml b/hw1/conf/config.yaml
index a1df6c2..839d79f 100644
--- a/hw1/conf/config.yaml
+++ b/hw1/conf/config.yaml
@@ -3,12 +3,12 @@ env:
   expert_data: ../../../hw1/roble/expert_data/expert_data_Ant-v2.pkl  # Relative to where you're running this script from
   exp_name: "bob"
   env_name: Ant-v2 # choices are [Ant-v2, Humanoid-v2, Walker2d-v2, HalfCheetah-v2, Hopper-v2]
-  max_episode_length: 100 
+  max_episode_length: 1000
   render: false
   
 alg:
   num_rollouts: 5
-  do_dagger: true
+  do_dagger: False
   num_agent_train_steps_per_iter: 1000 # number of gradient steps for training policy (per iter in n_iter)
   n_iter: 10
   batch_size: 1000 # training data collected (in the env) during each iteration
diff --git a/hw1/roble/infrastructure/pytorch_util.py b/hw1/roble/infrastructure/pytorch_util.py
index f61a63f..21b1b00 100644
--- a/hw1/roble/infrastructure/pytorch_util.py
+++ b/hw1/roble/infrastructure/pytorch_util.py
@@ -16,7 +16,6 @@ _str_to_activation = {
     'identity': nn.Identity(),
 }
 
-
 def build_mlp(
         input_size: int,
         output_size: int,
@@ -47,10 +46,22 @@ def build_mlp(
 
     # TODO: return a MLP. This should be an instance of nn.Module
     # Note: nn.Sequential is an instance of nn.Module.
-    raise NotImplementedError
 
-device = None
+    modules = []
+    modules.append(nn.Linear(input_size, size))
+    modules.append(activation)
+    
+    for layer in range(n_layers):
+        modules.append(nn.Linear(size, size))
+        modules.append(activation)
 
+    modules.append(nn.Linear(size, output_size))
+    modules.append(output_activation)
+
+    sequential = nn.Sequential(*modules)
+    return sequential
+
+device = None
 
 def init_gpu(use_gpu=True, gpu_id=0):
     global device
diff --git a/hw1/roble/infrastructure/replay_buffer.py b/hw1/roble/infrastructure/replay_buffer.py
index 0818486..68f04b6 100644
--- a/hw1/roble/infrastructure/replay_buffer.py
+++ b/hw1/roble/infrastructure/replay_buffer.py
@@ -76,7 +76,9 @@ class ReplayBuffer(object):
         ## HINT 1: use np.random.permutation to sample random indices
         ## HINT 2: return corresponding data points from each array (i.e., not different indices from each array)
         ## HINT 3: look at the sample_recent_data function below
-        raise NotImplementedError #return TODO, TODO, TODO, TODO, TODO
+
+        idcs = np.random.permutation(np.arange(self.obs.shape[0]))
+        return self.obs[:batch_size], self.acs[:batch_size], self.rews[:batch_size], self.next_obs[:batch_size], self.terminals[:batch_size]
 
     def sample_recent_data(self, batch_size=1):
         return (
diff --git a/hw1/roble/infrastructure/rl_trainer.py b/hw1/roble/infrastructure/rl_trainer.py
index e9822be..268ea54 100644
--- a/hw1/roble/infrastructure/rl_trainer.py
+++ b/hw1/roble/infrastructure/rl_trainer.py
@@ -165,13 +165,20 @@ class RL_Trainer(object):
             # ``` return loaded_paths, 0, None ```
 
             # (2) collect `self.params['batch_size']` transitions
+
+        if itr == 0:
+            print("first training iteration. loading expert data")
+            with open(load_initial_expertdata, 'rb') as f:
+                loaded_paths = pickle.load(f)
+            return loaded_paths, 0, None
+
         # TODO collect `batch_size` samples to be used for training
         # HINT1: use sample_trajectories from utils
         # HINT2: you want each of these collected rollouts to be of length self.params['ep_len']
 
         print("\nCollecting data to be used for training...")
 
-        paths, envsteps_this_batch = TODO
+        paths, envsteps_this_batch = utils.sample_trajectories(self.env, collect_policy, batch_size, self.params['env']['max_episode_length'])
 
         # collect more rollouts with the same policy, to be saved as videos in tensorboard
         # note: here, we collect MAX_NVIDEO rollouts, each of length MAX_VIDEO_LEN
@@ -192,12 +199,12 @@ class RL_Trainer(object):
             # TODO sample some data from the data buffer
             # HINT1: use the agent's sample function
             # HINT2: how much data = self.params['train_batch_size']
-            ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = TODO
+            ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = self.agent.sample(self.params['alg']['train_batch_size'])
 
             # TODO use the sampled data to train an agent
             # HINT: use the agent's train function
             # HINT: keep the agent's training log for debugging
-            train_log = TODO
+            train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)
             all_logs.append(train_log)
         return all_logs
 
@@ -207,7 +214,8 @@ class RL_Trainer(object):
         # TODO relabel collected obsevations (from our policy) with labels from an expert policy
         # HINT: query the policy (using the get_action function) with paths[i]["observation"]
         # and replace paths[i]["action"] with these expert labels
-
+        for i in range(len(paths)):
+            paths[i]["action"] = expert_policy.get_action(paths[i]["observation"])
         return paths
 
     ####################################
diff --git a/hw1/roble/infrastructure/utils.py b/hw1/roble/infrastructure/utils.py
index d5ed7ca..138a540 100644
--- a/hw1/roble/infrastructure/utils.py
+++ b/hw1/roble/infrastructure/utils.py
@@ -6,7 +6,7 @@ import time
 
 def sample_trajectory(env, policy, max_path_length, render=False, render_mode=('rgb_array')):
     # initialize env for the beginning of a new rollout
-    ob = TODO # HINT: should be the output of resetting the env
+    ob = env.reset() # HINT: should be the output of resetting the env
     obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []
     steps = 0
     while True:
@@ -24,7 +24,7 @@ def sample_trajectory(env, policy, max_path_length, render=False, render_mode=('
                 time.sleep(env.model.opt.timestep)
         # use the most recent ob to decide what to do
         obs.append(ob)
-        ac = TODO # HINT: query the policy's get_action function
+        ac = policy.get_action(ob) # HINT: query the policy's get_action function
         ac = ac[0]
         acs.append(ac)
         ob, rew, done, _ = env.step(ac)
@@ -39,7 +39,7 @@ def sample_trajectory(env, policy, max_path_length, render=False, render_mode=('
 
         # TODO end the rollout if the rollout ended
         # HINT: rollout can end due to done, or due to max_path_length
-        rollout_done = TODO # HINT: this is either 0 or 1
+        rollout_done = done or steps == max_path_length # HINT: this is either 0 or 1
         terminals.append(rollout_done)
 
         if rollout_done:
@@ -58,7 +58,9 @@ def sample_trajectories(env, policy, min_timesteps_per_batch, max_path_length, r
     timesteps_this_batch = 0
     paths = []
     while timesteps_this_batch < min_timesteps_per_batch:
-        TODO
+        traj = sample_trajectory(env, policy, max_path_length, render=render, render_mode=render_mode)
+        paths.append(traj)
+        timesteps_this_batch += get_pathlength(traj)
     return paths, timesteps_this_batch
 
 def sample_n_trajectories(env, policy, ntraj, max_path_length, render=False, render_mode=('rgb_array')):
@@ -69,7 +71,9 @@ def sample_n_trajectories(env, policy, ntraj, max_path_length, render=False, ren
         Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths
     """
     paths = []
-    TODO
+    for i in range(ntraj):
+        traj = sample_trajectory(env, policy, max_path_length, render=render, render_mode=render_mode)
+        paths.append(traj)
     return paths
 
 ############################################
diff --git a/plot_curves.py b/plot_curves.py
new file mode 100644
index 0000000..cb7b5bf
--- /dev/null
+++ b/plot_curves.py
@@ -0,0 +1,53 @@
+import matplotlib.pyplot as plt
+import numpy as np
+
+dagger_data = "/home/roger/Desktop/ift6163_homeworks_2023/outputs/2023-01-22/09-09-47/data/q2_bob_Walker2d-v2_22-01-2023_09-09-47/log_file.log"
+behaviour_cloning_data = "/home/roger/Desktop/ift6163_homeworks_2023/outputs/2023-01-22/08-50-23/data/q1_bob_Walker2d-v2_22-01-2023_08-50-23/log_file.log"
+
+# We are only going to keep this info
+keep_phrases_returns = ["Eval_AverageReturn"]
+keep_phrases_expert = ["Initial_DataCollection_AverageReturn"]
+keep_phrases_errors = ["Eval_StdReturn"]
+
+# read behaviour cloning performance from log file
+with open(behaviour_cloning_data) as f:
+    f = f.readlines()
+for line in f:
+    for phrase in keep_phrases_returns:
+        if phrase in line:
+            bh_performance = float(line.split(":")[2][1:-2])
+            break
+    for phrase in keep_phrases_expert:
+        if phrase in line:
+            expert_performance = float(line.split(":")[2][1:-2])
+            break
+
+# read Dagger retruns and std error across iterations
+returns = []
+errors = []
+with open(dagger_data) as f:
+    f = f.readlines()
+for line in f:
+    for phrase in keep_phrases_returns:
+        if phrase in line:
+            returns.append(float(line.split(":")[2][1:-2]))
+            break
+    for phrase in keep_phrases_errors:
+        if phrase in line:
+            errors.append(float(line.split(":")[2][1:-2]))
+            break
+
+returns = np.concatenate([np.array([0]), np.array(returns)])
+errors = np.concatenate([np.array([0]), np.array(errors)])
+
+# generate the plot
+x_data = np.arange(len(returns))
+plt.figure()
+plt.plot(x_data, returns, label="Dagger")
+plt.fill_between(x_data, returns - errors, returns + errors, alpha = 0.2)
+plt.axhline(y = bh_performance, color = 'r', linestyle = '-', label="Behaviour Cloning")
+plt.axhline(y = expert_performance, color = 'g', linestyle = '-', label="Expert Policy")
+plt.xlabel('Number of iterations')
+plt.ylabel('Avg. Eval return')
+plt.legend(loc="center right")
+plt.savefig("dagger_curves.jpg")
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
index 4a7b752..534bfb6 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -18,4 +18,4 @@ Hydra==2.5
 hydra-core==1.1.1
 atari_py==0.2.6
 protobuf==3.20.1
-cudatoolkit==11.3.1
+#cudatoolkit==11.3.1
diff --git a/Dockerfile b/Dockerfile
index 733c9b8..4edf54e 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -104,7 +104,6 @@ RUN ls
 COPY requirements.txt requirements.txt
 RUN pip install -r requirements.txt
 
-# RUN conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
-
+#RUN conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia
 
 RUN ls
diff --git a/Dockerfile2 b/Dockerfile2
new file mode 100644
index 0000000..5c86989
--- /dev/null
+++ b/Dockerfile2
@@ -0,0 +1,111 @@
+# Base container that includes all dependencies but not the actual repo
+
+ARG UBUNTU_VERSION=20.04
+ARG ARCH=
+ARG CUDA=11.4.0
+
+# RUN echo nvidia/cudagl${ARCH:+-$ARCH}:${CUDA}-base-ubuntu${UBUNTU_VERSION}
+# FROM nvidia/cudagl${ARCH:+-$ARCH}:${CUDA}-base-ubuntu${UBUNTU_VERSION} as base
+#FROM nvidia/cudagl:11.4.2-base-ubuntu20.04 as base
+FROM liqingya/mujoco:dmc-atari-py36-torch1.7-cu110
+# ARCH and CUDA are specified again because the FROM directive resets ARGs
+# (but their default value is retained if set previously)
+
+ARG UBUNTU_VERSION
+ARG ARCH
+ARG CUDA
+ARG CUDNN=7.6.5.32-1
+
+SHELL ["/bin/bash", "-c"]
+
+ENV DEBIAN_FRONTEND="noninteractive"
+# See http://bugs.python.org/issue19846
+ENV LANG=C.UTF-8 LC_ALL=C.UTF-8
+ENV PATH /opt/conda/bin:$PATH
+
+# install anaconda
+RUN apt-get update --fix-missing && apt-get install -y wget bzip2 ca-certificates \
+    libglib2.0-0 libxext6 libsm6 libxrender1 \
+    git mercurial subversion
+    
+# NOTE: we don't use TF so might not need some of these
+# ========== Tensorflow dependencies ==========
+RUN apt-get update \
+    && apt-get install -y --no-install-recommends \
+        build-essential \
+        libfreetype6-dev \
+        libhdf5-serial-dev \
+        libzmq3-dev \
+        pkg-config \
+        software-properties-common \
+        zip \
+        unzip \
+    && apt-get clean \
+    && rm -rf /var/lib/apt/lists/*
+
+SHELL ["/bin/bash", "-c"]
+
+RUN apt-get update -y
+# RUN apt-get install -y python3-dev python3-pip
+RUN apt-get update --fix-missing
+RUN apt-get install -y wget bzip2 ca-certificates git vim
+RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
+        build-essential \
+        premake4 \
+        git \
+        curl \
+        vim \
+        ffmpeg \
+	    libgl1-mesa-dev \
+	    libgl1-mesa-glx \
+	    libglew-dev \
+	    libosmesa6-dev \
+	    libxrender-dev \
+	    libsm6 libxext6 \
+        unzip \
+        patchelf \
+        ffmpeg \
+        libxrandr2 \
+        libxinerama1 \
+        libxcursor1 \
+        python3-dev python3-pip graphviz \
+        freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev libglew1.6-dev mesa-utils
+        
+# Not sure why this is needed
+ENV LANG C.UTF-8
+
+# Not sure what this is fixing
+# COPY ./files/Xdummy /usr/local/bin/Xdummy
+# RUN chmod +x /usr/local/bin/Xdummy
+        
+ENV PATH /opt/conda/bin:$PATH
+RUN wget --quiet https://repo.anaconda.com/archive/Anaconda2-2019.10-Linux-x86_64.sh -O /tmp/miniconda.sh && \
+    /bin/bash /tmp/miniconda.sh -b -p /opt/conda && \
+    rm /tmp/miniconda.sh && \
+    ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh && \
+    echo ". /opt/conda/etc/profile.d/conda.sh" >> /etc/bash.bashrc
+
+RUN conda update -y --name base conda && conda clean --all -y
+
+RUN conda create --name roble python=3.7 pip
+RUN echo "source activate roble" >> ~/.bashrc
+## Make it so you can install things to the correct version of pip
+ENV PATH /opt/conda/envs/roble/bin:$PATH
+RUN source activate roble
+
+RUN mkdir /root/playground
+
+# make sure your domain is accepted
+# RUN touch /root/.ssh/known_hosts
+RUN mkdir /root/.ssh
+RUN ssh-keyscan github.com >> /root/.ssh/known_hosts
+
+RUN ls
+COPY requirements.txt requirements.txt
+RUN pip install -r requirements.txt
+
+RUN conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia
+
+RUN pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
+
+RUN ls
\ No newline at end of file
diff --git a/conf/config_hw1.yaml b/conf/config_hw1.yaml
index 7ba75d1..15075a4 100644
--- a/conf/config_hw1.yaml
+++ b/conf/config_hw1.yaml
@@ -1,34 +1,30 @@
-env: 
-  expert_policy_file: ../../../hw1/roble/policies/experts/Ant.pkl # Relative to where you're running this script from 
-  expert_data: ../../../hw1/roble/expert_data/expert_data_Ant-v2.pkl  # Relative to where you're running this script from
-  exp_name: "bob"
-  env_name: Ant-v2 # choices are [Ant-v2, Humanoid-v2, Walker2d-v2, HalfCheetah-v2, Hopper-v2]
-  max_episode_length: 100 
-  render: false
-  
+env:
+  expert_policy_file: ../../../hw1/roble/policies/experts/Walker2d.pkl
+  expert_data: ../../../hw1/roble/expert_data/expert_data_Walker2d-v2.pkl
+  exp_name: bob
+  env_name: Walker2d-v2
+  max_episode_length: 1000
+  render: true
 alg:
   num_rollouts: 5
   do_dagger: true
-  num_agent_train_steps_per_iter: 1000 # number of gradient steps for training policy (per iter in n_iter)
-  n_iter: 10
-  batch_size: 1000 # training data collected (in the env) during each iteration
-  eval_batch_size: 1000 # eval data collected (in the env) for logging metrics
-  train_batch_size: 100 # number of sampled data points to be used per gradient/train step
-  n_layers: 2 # Network depths
-  network_width: 64 # The width of the network layers
-  learning_rate: 5e-3 # THe learning rate for BC
-  max_replay_buffer_size: 1000000 ## Size of the replay buffer
-  use_gpu: False
-  which_gpu: 0 # The index for the GPU (the computer you use may have more than one)
-  discrete: False
-  ac_dim: 0 ## This will be overridden in the code
-  ob_dim: 0 ## This will be overridden in the code
+  num_agent_train_steps_per_iter: 1000
+  n_iter: 21
+  batch_size: 1000
+  eval_batch_size: 10000
+  train_batch_size: 1024
+  n_layers: 2
+  network_width: 64
+  learning_rate: 0.005
+  max_replay_buffer_size: 1000000
+  use_gpu: false
+  which_gpu: 0
+  discrete: false
+  ac_dim: 0
+  ob_dim: 0
 
 logging:
-  video_log_freq: 5 # How often to generate a video to log/
-  scalar_log_freq: 1 # How often to log training information and run evaluation during training.
-  save_params: true # Should the parameters given to the script be saved? (Always...)
+  video_log_freq: 5
+  scalar_log_freq: 1
+  save_params: true
   random_seed: 1234
-  
-  
-
diff --git a/hw1/conf/config.yaml b/hw1/conf/config.yaml
index a1df6c2..839d79f 100644
--- a/hw1/conf/config.yaml
+++ b/hw1/conf/config.yaml
@@ -3,12 +3,12 @@ env:
   expert_data: ../../../hw1/roble/expert_data/expert_data_Ant-v2.pkl  # Relative to where you're running this script from
   exp_name: "bob"
   env_name: Ant-v2 # choices are [Ant-v2, Humanoid-v2, Walker2d-v2, HalfCheetah-v2, Hopper-v2]
-  max_episode_length: 100 
+  max_episode_length: 1000
   render: false
   
 alg:
   num_rollouts: 5
-  do_dagger: true
+  do_dagger: False
   num_agent_train_steps_per_iter: 1000 # number of gradient steps for training policy (per iter in n_iter)
   n_iter: 10
   batch_size: 1000 # training data collected (in the env) during each iteration
diff --git a/hw1/roble/infrastructure/pytorch_util.py b/hw1/roble/infrastructure/pytorch_util.py
index f61a63f..21b1b00 100644
--- a/hw1/roble/infrastructure/pytorch_util.py
+++ b/hw1/roble/infrastructure/pytorch_util.py
@@ -16,7 +16,6 @@ _str_to_activation = {
     'identity': nn.Identity(),
 }
 
-
 def build_mlp(
         input_size: int,
         output_size: int,
@@ -47,10 +46,22 @@ def build_mlp(
 
     # TODO: return a MLP. This should be an instance of nn.Module
     # Note: nn.Sequential is an instance of nn.Module.
-    raise NotImplementedError
 
-device = None
+    modules = []
+    modules.append(nn.Linear(input_size, size))
+    modules.append(activation)
+    
+    for layer in range(n_layers):
+        modules.append(nn.Linear(size, size))
+        modules.append(activation)
 
+    modules.append(nn.Linear(size, output_size))
+    modules.append(output_activation)
+
+    sequential = nn.Sequential(*modules)
+    return sequential
+
+device = None
 
 def init_gpu(use_gpu=True, gpu_id=0):
     global device
diff --git a/hw1/roble/infrastructure/replay_buffer.py b/hw1/roble/infrastructure/replay_buffer.py
index 0818486..68f04b6 100644
--- a/hw1/roble/infrastructure/replay_buffer.py
+++ b/hw1/roble/infrastructure/replay_buffer.py
@@ -76,7 +76,9 @@ class ReplayBuffer(object):
         ## HINT 1: use np.random.permutation to sample random indices
         ## HINT 2: return corresponding data points from each array (i.e., not different indices from each array)
         ## HINT 3: look at the sample_recent_data function below
-        raise NotImplementedError #return TODO, TODO, TODO, TODO, TODO
+
+        idcs = np.random.permutation(np.arange(self.obs.shape[0]))
+        return self.obs[:batch_size], self.acs[:batch_size], self.rews[:batch_size], self.next_obs[:batch_size], self.terminals[:batch_size]
 
     def sample_recent_data(self, batch_size=1):
         return (
diff --git a/hw1/roble/infrastructure/rl_trainer.py b/hw1/roble/infrastructure/rl_trainer.py
index e9822be..268ea54 100644
--- a/hw1/roble/infrastructure/rl_trainer.py
+++ b/hw1/roble/infrastructure/rl_trainer.py
@@ -165,13 +165,20 @@ class RL_Trainer(object):
             # ``` return loaded_paths, 0, None ```
 
             # (2) collect `self.params['batch_size']` transitions
+
+        if itr == 0:
+            print("first training iteration. loading expert data")
+            with open(load_initial_expertdata, 'rb') as f:
+                loaded_paths = pickle.load(f)
+            return loaded_paths, 0, None
+
         # TODO collect `batch_size` samples to be used for training
         # HINT1: use sample_trajectories from utils
         # HINT2: you want each of these collected rollouts to be of length self.params['ep_len']
 
         print("\nCollecting data to be used for training...")
 
-        paths, envsteps_this_batch = TODO
+        paths, envsteps_this_batch = utils.sample_trajectories(self.env, collect_policy, batch_size, self.params['env']['max_episode_length'])
 
         # collect more rollouts with the same policy, to be saved as videos in tensorboard
         # note: here, we collect MAX_NVIDEO rollouts, each of length MAX_VIDEO_LEN
@@ -192,12 +199,12 @@ class RL_Trainer(object):
             # TODO sample some data from the data buffer
             # HINT1: use the agent's sample function
             # HINT2: how much data = self.params['train_batch_size']
-            ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = TODO
+            ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch = self.agent.sample(self.params['alg']['train_batch_size'])
 
             # TODO use the sampled data to train an agent
             # HINT: use the agent's train function
             # HINT: keep the agent's training log for debugging
-            train_log = TODO
+            train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)
             all_logs.append(train_log)
         return all_logs
 
@@ -207,7 +214,8 @@ class RL_Trainer(object):
         # TODO relabel collected obsevations (from our policy) with labels from an expert policy
         # HINT: query the policy (using the get_action function) with paths[i]["observation"]
         # and replace paths[i]["action"] with these expert labels
-
+        for i in range(len(paths)):
+            paths[i]["action"] = expert_policy.get_action(paths[i]["observation"])
         return paths
 
     ####################################
diff --git a/hw1/roble/infrastructure/utils.py b/hw1/roble/infrastructure/utils.py
index d5ed7ca..138a540 100644
--- a/hw1/roble/infrastructure/utils.py
+++ b/hw1/roble/infrastructure/utils.py
@@ -6,7 +6,7 @@ import time
 
 def sample_trajectory(env, policy, max_path_length, render=False, render_mode=('rgb_array')):
     # initialize env for the beginning of a new rollout
-    ob = TODO # HINT: should be the output of resetting the env
+    ob = env.reset() # HINT: should be the output of resetting the env
     obs, acs, rewards, next_obs, terminals, image_obs = [], [], [], [], [], []
     steps = 0
     while True:
@@ -24,7 +24,7 @@ def sample_trajectory(env, policy, max_path_length, render=False, render_mode=('
                 time.sleep(env.model.opt.timestep)
         # use the most recent ob to decide what to do
         obs.append(ob)
-        ac = TODO # HINT: query the policy's get_action function
+        ac = policy.get_action(ob) # HINT: query the policy's get_action function
         ac = ac[0]
         acs.append(ac)
         ob, rew, done, _ = env.step(ac)
@@ -39,7 +39,7 @@ def sample_trajectory(env, policy, max_path_length, render=False, render_mode=('
 
         # TODO end the rollout if the rollout ended
         # HINT: rollout can end due to done, or due to max_path_length
-        rollout_done = TODO # HINT: this is either 0 or 1
+        rollout_done = done or steps == max_path_length # HINT: this is either 0 or 1
         terminals.append(rollout_done)
 
         if rollout_done:
@@ -58,7 +58,9 @@ def sample_trajectories(env, policy, min_timesteps_per_batch, max_path_length, r
     timesteps_this_batch = 0
     paths = []
     while timesteps_this_batch < min_timesteps_per_batch:
-        TODO
+        traj = sample_trajectory(env, policy, max_path_length, render=render, render_mode=render_mode)
+        paths.append(traj)
+        timesteps_this_batch += get_pathlength(traj)
     return paths, timesteps_this_batch
 
 def sample_n_trajectories(env, policy, ntraj, max_path_length, render=False, render_mode=('rgb_array')):
@@ -69,7 +71,9 @@ def sample_n_trajectories(env, policy, ntraj, max_path_length, render=False, ren
         Hint1: use sample_trajectory to get each path (i.e. rollout) that goes into paths
     """
     paths = []
-    TODO
+    for i in range(ntraj):
+        traj = sample_trajectory(env, policy, max_path_length, render=render, render_mode=render_mode)
+        paths.append(traj)
     return paths
 
 ############################################
diff --git a/plot_curves.py b/plot_curves.py
new file mode 100644
index 0000000..cb7b5bf
--- /dev/null
+++ b/plot_curves.py
@@ -0,0 +1,53 @@
+import matplotlib.pyplot as plt
+import numpy as np
+
+dagger_data = "/home/roger/Desktop/ift6163_homeworks_2023/outputs/2023-01-22/09-09-47/data/q2_bob_Walker2d-v2_22-01-2023_09-09-47/log_file.log"
+behaviour_cloning_data = "/home/roger/Desktop/ift6163_homeworks_2023/outputs/2023-01-22/08-50-23/data/q1_bob_Walker2d-v2_22-01-2023_08-50-23/log_file.log"
+
+# We are only going to keep this info
+keep_phrases_returns = ["Eval_AverageReturn"]
+keep_phrases_expert = ["Initial_DataCollection_AverageReturn"]
+keep_phrases_errors = ["Eval_StdReturn"]
+
+# read behaviour cloning performance from log file
+with open(behaviour_cloning_data) as f:
+    f = f.readlines()
+for line in f:
+    for phrase in keep_phrases_returns:
+        if phrase in line:
+            bh_performance = float(line.split(":")[2][1:-2])
+            break
+    for phrase in keep_phrases_expert:
+        if phrase in line:
+            expert_performance = float(line.split(":")[2][1:-2])
+            break
+
+# read Dagger retruns and std error across iterations
+returns = []
+errors = []
+with open(dagger_data) as f:
+    f = f.readlines()
+for line in f:
+    for phrase in keep_phrases_returns:
+        if phrase in line:
+            returns.append(float(line.split(":")[2][1:-2]))
+            break
+    for phrase in keep_phrases_errors:
+        if phrase in line:
+            errors.append(float(line.split(":")[2][1:-2]))
+            break
+
+returns = np.concatenate([np.array([0]), np.array(returns)])
+errors = np.concatenate([np.array([0]), np.array(errors)])
+
+# generate the plot
+x_data = np.arange(len(returns))
+plt.figure()
+plt.plot(x_data, returns, label="Dagger")
+plt.fill_between(x_data, returns - errors, returns + errors, alpha = 0.2)
+plt.axhline(y = bh_performance, color = 'r', linestyle = '-', label="Behaviour Cloning")
+plt.axhline(y = expert_performance, color = 'g', linestyle = '-', label="Expert Policy")
+plt.xlabel('Number of iterations')
+plt.ylabel('Avg. Eval return')
+plt.legend(loc="center right")
+plt.savefig("dagger_curves.jpg")
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
index 4a7b752..534bfb6 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -18,4 +18,4 @@ Hydra==2.5
 hydra-core==1.1.1
 atari_py==0.2.6
 protobuf==3.20.1
-cudatoolkit==11.3.1
+#cudatoolkit==11.3.1
